# Movies-ETL

## Overview of Project

### Purpose
A company named Amazing Prime has a dataset which they would like to keep up-to-date on a daily basis.  An employee (Britta) has requested my assistance to to create an automated pipeline that will Extract, Transform and Load (ETL) the data into exising PostgreSQL tables.  The existing code has been refactored to to take arguements of the three (3) file names that include Wikipedia data, Kaggle metadata and the MovieLens rating data.  The standard data analysis principles were used which includes; (1) Determine the number of rows and columns; (2) Data types used; and (3) Is the data readable?


## Resources

- Jupyter Notebook
- PostgreSQL (A Relational Database Management System)
- pgAdmin (Interface to interact with PostgreSQL)

- Datasets used for Analysis:

  Kaggle Data: [movies_metadata.csv](https://github.com/SheaButta/Pewlett-Hackard-Analysis/blob/main/Data/employees.csv)

  Ratings Data: [rating.csv](https://github.com/SheaButta/Pewlett-Hackard-Analysis/blob/main/Data/departments.csv)

  Wikipedia Data: [wikipedia-movies.json](https://github.com/SheaButta/Pewlett-Hackard-Analysis/blob/main/Data/dept_emp.csv)

  
 
 ----------------------------
 
